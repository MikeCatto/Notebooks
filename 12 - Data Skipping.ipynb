{"cells":[{"cell_type":"markdown","source":["# Data Skipping\n","\n","Data skipping information is collected **automatically** when you write data into a Delta Lake table. **_Delta Lake takes advantage_** of this information (minimum and maximum values for each column) at query time to provide faster queries. \n","\n","You do not need to configure data skipping; the feature is activated whenever applicable. However, its effectiveness depends on the layout of your data. For best results, apply Z-Ordering.\n","\n","Collecting statistics on a column containing long values such as string or binary is an expensive operation. To avoid collecting statistics on such columns you can configure the table property **delta.dataSkippingNumIndexedCols**. \n","\n","This property indicates the position index of a column in the table's schema. All columns with a position index less than the **delta.dataSkippingNumIndexedCols** property will have statistics collected. \n","\n","For the purposes of collecting statistics, each field within a nested column is considered as an individual column. To avoid collecting statistics on columns containing long values, either set the **delta.dataSkippingNumIndexedCols** property so that the long value columns are after this index in the table's schema, or move columns containing long strings to an index position greater than the **delta.dataSkippingNumIndexedCols** property by using **ALTER TABLE ALTER COLUMN**.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"21bca451-c1e4-4c36-89aa-4bddda98dad4"},{"cell_type":"markdown","source":["## Generate dummy data\n","\n","[Data Generator](https://github.com/databrickslabs/dbldatagen)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"441fe27e-464a-4fd2-bbb5-ee88b3de8462"},{"cell_type":"code","source":["%pip install dbldatagen\n","%pip install jmespath"],"outputs":[],"execution_count":null,"metadata":{},"id":"b80ebb64-6563-4e03-9b12-c3bb770750fa"},{"cell_type":"code","source":["import dbldatagen as dg\n","from pyspark.sql.types import IntegerType, FloatType, StringType\n","column_count = 35\n","data_rows = 1000\n","df_spec = (dg.DataGenerator(spark, name=\"test_data_set1\", rows=data_rows,\n","                                                  partitions=4)\n","           .withIdOutput()\n","           .withColumn(\"r\", FloatType(), \n","                            expr=\"floor(rand() * 350) * (86400 + 3600)\",\n","                            numColumns=column_count)\n","           .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200)\n","           .withColumn(\"code2\", IntegerType(), minValue=0, maxValue=10)\n","           .withColumn(\"code3\", StringType(), values=['a', 'b', 'c'])\n","           .withColumn(\"code4\", StringType(), values=['a', 'b', 'c'], \n","                          random=True)\n","           .withColumn(\"code5\", StringType(), values=['a', 'b', 'c'], \n","                          random=True, weights=[9, 1, 1])\n"," \n","           )\n","\n","delta_table_name = 'demo.data_skipping_demo'\n","spark.sql(f\"DROP TABLE IF EXISTS {delta_table_name}\")\n","                         \n","df = df_spec.build()\n","df.write.format(\"delta\").saveAsTable(delta_table_name)                        "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5818ae8f-04cf-4eb4-b0cb-80c92a1ab7f1"},{"cell_type":"markdown","source":["## Checking stats\n","\n","As you can see there are mode than 32 columns in the table, So only the first 32 columns will have statistics created. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f278f2ce-cc5c-4f06-8948-0ce0dbe84b7b"},{"cell_type":"code","source":["%%sql\n","DESCRIBE demo.data_skipping_demo"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"d1fcd7a3-2f99-4fcd-b354-3a18c2e58aa1"},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField,BooleanType,LongType,StringType\n","from pyspark.sql.functions import col, from_json\n","deltalog = spark.read.json(\"Tables/data_skipping_demo/_delta_log/00000000000000000000.json\")\n","schema= StructType([\n","    StructField('numRecords',LongType(), True), \n","    StructField('minValues', StringType(), True), \n","    StructField('maxValues', StringType(), True), \n","    StructField('nullCount', StringType(), True)\n","    ])\n","\n","df_add = deltalog.select(from_json(col('add.stats'),schema).alias('stats')).select(['stats.numRecords','stats.minValues','stats.maxValues','stats.nullCount']).where(\"add is not null\")\n","display(df_add)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"run_control":{"frozen":false},"editable":true},"id":"7e0fe12c-0c13-4c75-a079-422bb1042031"},{"cell_type":"markdown","source":["## Change column \n","\n","Now, let's change a column that is "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"01ba83a1-1d2b-4ea9-9e60-249af2b2376e"},{"cell_type":"code","source":["%%sql\n","ALTER TABLE data_skipping_demo CHANGE COLUMN code5 AFTER r_0"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"d21f9222-d8bf-473f-859f-bd87109d28ce"},{"cell_type":"code","source":["%%sql\n","DESCRIBE demo.data_skipping_demo"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"f1fdde46-f2dc-4995-9d78-fe7a80fa65b7"},{"cell_type":"markdown","source":["## Checking Delta log"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7831841a-9d31-4a77-816e-1659330919b1"},{"cell_type":"code","source":["import delta\n","\n","delta_info = delta_info = delta.DeltaTable.forName(spark, \"demo.data_skipping_demo\")\n","\n","display(delta_info.history())"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"07777b1d-0291-4073-886f-4ae08bb76de6"},{"cell_type":"code","source":["deltalog = spark.read.json(\"Tables/data_skipping_demo/_delta_log/00000000000000000001.json\")\n","display(deltalog)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"2f3bb06f-21bd-40d9-aef6-cc2c9a0c432b"},{"cell_type":"markdown","source":["## Appending new data"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"807a6bb7-b2dd-4f9d-a284-79f85cd504e8"},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType, FloatType, StringType\n","column_count = 35\n","data_rows = 10\n","df_spec = (dg.DataGenerator(spark, name=\"test_data_set1\", rows=data_rows,\n","                                                  partitions=4)\n","           .withIdOutput()\n","           .withColumn(\"r\", FloatType(), \n","                            expr=\"floor(rand() * 350) * (86400 + 3600)\",\n","                            numColumns=column_count)\n","           .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200)\n","           .withColumn(\"code2\", IntegerType(), minValue=0, maxValue=10)\n","           .withColumn(\"code3\", StringType(), values=['a', 'b', 'c'])\n","           .withColumn(\"code4\", StringType(), values=['a', 'b', 'c'], \n","                          random=True)\n","           .withColumn(\"code5\", StringType(), values=['a', 'b', 'c'], \n","                          random=True, weights=[9, 1, 1])\n"," \n","           )\n","                            \n","df = df_spec.build()\n","df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data_skipping_demo\")    "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"01c92c12-3bcc-4900-869f-2732d60ef8fa"},{"cell_type":"code","source":["display(delta_info.history())"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"3fcfab92-dd7a-494c-83aa-3408ca440897"},{"cell_type":"markdown","source":["New stats are collected only for new data.\n","\n","Column code5 has now stats however the last column with stats has changed."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b3d626b2-54ac-48fa-ad28-9e51bb898849"},{"cell_type":"code","source":["deltalog = spark.read.json(\"Tables/data_skipping_demo/_delta_log/00000000000000000002.json\")\n","\n","schema = StructType([StructField(\"numRecords\", IntegerType(), False),\n","                StructField(\"minValues\", StringType(), False),\n","                StructField(\"maxValues\", StringType(), False), \n","                StructField(\"nullCount\", StringType(), False)])\n","\n","deltalog = deltalog.withColumn(\"parsed_stats\", from_json(deltalog[\"add.stats\"], schema))\n","\n","display(deltalog.select(\"add.path\", \"parsed_stats.numRecords\",\"parsed_stats.minValues\",\"parsed_stats.maxValues\",\"parsed_stats.nullCount\").where(\"add is not null\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"96ca7523-0995-47ed-8228-f953802899d4"},{"cell_type":"markdown","source":["## Increasing / Decreasing number of columns stats"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"61fe8280-3c8f-4ab9-8976-622c8fcd7e03"},{"cell_type":"code","source":["%%sql\n","ALTER TABLE demo.data_skipping_demo SET TBLPROPERTIES (\"delta.dataSkippingNumIndexedCols\" = 5)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false},"id":"5a32d149-d767-4b0a-a586-1f8f077dded0"},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType, FloatType, StringType\n","column_count = 35\n","data_rows = 10\n","df_spec = (dg.DataGenerator(spark, name=\"test_data_set1\", rows=data_rows,\n","                                                  partitions=4)\n","           .withIdOutput()\n","           .withColumn(\"r\", FloatType(), \n","                            expr=\"floor(rand() * 350) * (86400 + 3600)\",\n","                            numColumns=column_count)\n","           .withColumn(\"code1\", IntegerType(), minValue=100, maxValue=200)\n","           .withColumn(\"code2\", IntegerType(), minValue=0, maxValue=10)\n","           .withColumn(\"code3\", StringType(), values=['a', 'b', 'c'])\n","           .withColumn(\"code4\", StringType(), values=['a', 'b', 'c'], \n","                          random=True)\n","           .withColumn(\"code5\", StringType(), values=['a', 'b', 'c'], \n","                          random=True, weights=[9, 1, 1])\n"," \n","           )\n","                            \n","df = df_spec.build()\n","df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data_skipping_demo\")    "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c87241dc-447f-4290-af26-14cce0a5e704"},{"cell_type":"code","source":["display(delta_info.history())"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"c6fc3a7c-2116-4f31-8b13-2d8ee68acec4"},{"cell_type":"code","source":["deltalog = spark.read.json(\"Tables/data_skipping_demo/_delta_log/00000000000000000004.json\")\n","\n","schema = StructType([StructField(\"numRecords\", IntegerType(), False),\n","                StructField(\"minValues\", StringType(), False),\n","                StructField(\"maxValues\", StringType(), False), \n","                StructField(\"nullCount\", StringType(), False)])\n","\n","deltalog = deltalog.withColumn(\"parsed_stats\", from_json(deltalog[\"add.stats\"], schema))\n","\n","display(deltalog.select(\"add.path\", \"parsed_stats.numRecords\",\"parsed_stats.minValues\",\"parsed_stats.maxValues\",\"parsed_stats.nullCount\").where(\"add is not null\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"044b6ab4-c388-4d79-a27f-699b7315ebb9"},{"cell_type":"markdown","source":["# Clean up"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"879359f1-7fdb-4b8c-b66f-18267dd760fb"},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS demo.data_skipping_demo\")  "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"444c4d9b-b668-448a-89cd-c2fc08e81394"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"notebook_environment":{},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}