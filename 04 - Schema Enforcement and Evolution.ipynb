{"cells":[{"cell_type":"markdown","source":["# Schema Enforcement\n","\n","**Schema enforcement**, also known as _**schema validation**_, is a safeguard in Delta Lake that ensures data quality by rejecting writes to a table that do not match the table's schema. Like the front desk manager at a busy restaurant that only accepts reservations, it checks to see whether each column in data inserted into the table is on its list of expected columns (in other words, whether each one has a \"reservation\"), and rejects any writes with columns that aren't on the list."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e5f3134a-c4e8-4d3c-b046-d4ec41401ac5"},{"cell_type":"markdown","source":["## Schema Validation\n","Delta Lake automatically validates that the schema of the dataframe being written is compatible with the schema of the table. Delta Lake uses the following rules to determine whether a write from a dataframe to a table is compatible:\n","\n","- **All dataframe columns must exist in the target table**. If there are columns in the dataframe not present in the table, an exception is raised. Columns present in the table but not in the dataframe are set to null.\n","- **Dataframe column data types must match the column data types in the target table**. If they don’t match, an exception is raised.\n","- **Dataframe column names cannot differ only by case.** This means that you cannot have columns such as “Foo” and “foo” defined in the same table. \n","\n","While you can use Spark in case sensitive or insensitive (default) mode, Parquet is case sensitive when storing and returning column information. \n","\n","Delta Lake is case-preserving but insensitive when storing the schema and has this restriction to avoid potential mistakes, data corruption, or loss issues.\n","Delta Lake support DDL to add new columns explicitly and the ability to update schema automatically."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c616536b-f4c9-4b47-8793-796606fffed5"},{"cell_type":"code","source":["# Generate dummy data\n","\n","from pyspark.sql.functions import expr, lit, col\n","from pyspark.sql.types import *\n","from datetime import date\n","\n","\n","df = spark.range(5) \\\n","  .selectExpr(\"if(id % 2 = 0, 'Open', 'Close') as action\") \\\n","  .withColumn(\"date\", expr(\"cast(concat('2023-06-', cast(rand(5) * 30 as int) + 1) as date)\")) \\\n","  .withColumn(\"device_id\", expr(\"cast(rand(5) * 100 as int)\"))\n","\n","spark.sql(\"DROP TABLE IF EXISTS demo.device\")\n","\n","delta_table_name = 'device'\n","df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_name)"],"outputs":[],"execution_count":null,"metadata":{"run_control":{"frozen":false},"editable":false},"id":"cf9eebe7-911e-45e9-b0a0-955514d93a82"},{"cell_type":"markdown","source":["Showing the **current table schema**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"895a95ed-1648-4a4b-9b0a-7f1b75d22886"},{"cell_type":"code","source":["%%sql \n","\n","DESCRIBE TABLE demo.device  "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false,"run_control":{"frozen":false},"editable":false},"id":"77d85a23-2fc3-47c8-a233-d1c487df2deb"},{"cell_type":"markdown","source":["Let's perform the same operations and and see how it works \n","- **Appending** some data that matches the table schema"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"af6d0f8e-cbbe-4457-8a04-3b75209584f4"},{"cell_type":"code","source":["deviceSchema = StructType([StructField(\"action\", StringType(), False),\n","  StructField(\"date\", DateType(), False),\n","  StructField(\"device_id\", IntegerType(), False),\n","  ])\n","\n","data = [\n","        ('In Progress', date.today(), -1)\n","    ]  \n","\n","new_device = spark.createDataFrame(data=data,schema=deviceSchema)\n","\n","# insert a new row into delta table\n","new_device.write.format(\"delta\").mode(\"append\").saveAsTable(\"demo.device\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"editable":false},"id":"f1cfe42f-d5c1-4563-b29a-9f4c2e330de2"},{"cell_type":"markdown","source":["> OR"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"86196462-dbf0-487e-9e90-0173b87ab2e0"},{"cell_type":"code","source":["%%sql \n","INSERT INTO demo.device \n","SELECT 'In Progress', current_date(), -1"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false,"editable":false,"run_control":{"frozen":true}},"id":"6d80333c-0041-4394-aec3-b6f7202671af"},{"cell_type":"markdown","source":["- **Appending** some data that has a new column\n","- New dataframe contains a new column named \"location\""],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c720323c-c6e7-4d25-a45e-85fcb69dd315"},{"cell_type":"code","source":["deviceSchema = StructType([StructField(\"action\", StringType(), False),\n","  StructField(\"date\", DateType(), False),\n","  StructField(\"device_id\", IntegerType(), False),\n","  StructField(\"location\", StringType(), False) # new column\n","  ])\n","\n","data = [\n","        ('In Progress', date.today(), -1, \"Dummy location\")\n","    ]  \n","\n","new_device = spark.createDataFrame(data=data,schema=deviceSchema)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"editable":false},"id":"cb51a48a-bb3b-4454-8284-ba4377d30551"},{"cell_type":"markdown","source":["Rather than automatically adding the new columns, **Delta Lake enforces the schema** and stops the write from occurring. \n","\n","To help identify which column(s) caused the mismatch, Spark **prints out both schemas** in the stack trace for comparison"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f20bdd65-c1ec-468c-b2fa-f0c6082ec075"},{"cell_type":"code","source":["# An exception will be thrown: A schema mismatch detected when writing to the Delta table\n","\n","new_device.write.format(\"delta\") \\\n","                .mode(\"append\") \\\n","                .saveAsTable(\"demo.device\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"editable":true,"run_control":{"frozen":false}},"id":"2b0ba70d-cdae-4d00-b98b-df118fa535f7"},{"cell_type":"markdown","source":["## Why is schema enforcement so important?\n","\n","Because it's such a stringent check, _**schema enforcement is an excellent tool**_ to use as a gatekeeper of a clean, fully transformed data set that is ready for production or consumption. It's typically enforced on tables that directly feed:\n","\n","- _Machine learning algorithms_\n","- _BI dashboards_\n","- _Data analytics and visualization tools_\n","- _Any production system requiring highly structured, strongly typed, semantic schema_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d6cef53d-bbfd-45ce-b728-6453446239dd"},{"cell_type":"markdown","source":["# Schema Evolution\n","\n","Schema evolution is a feature that **allows users to easily change** a table's current schema to _accommodate data that is changing over time_. Most commonly, it's used when performing an append or overwrite operation, to _**automatically adapt the schema**_ to include one or more new columns.\n","\n","You can append a dataframe with a different schema to the delta table by explicitly setting **mergeSchema** equal to **true**\n","\n","The following types of schema changes are eligible for schema evolution during table appends or overwrites:\n","\n","- _Adding new columns (this is the most common scenario)_\n","- _Changing of data types from NullType -> any other type, or upcasts from ByteType -> ShortType -> IntegerType_\n","\n","\n","Other changes, which are not eligible for schema evolution, require that the schema and data are overwritten by adding **.option(\"overwriteSchema\", \"true\")**. Those changes include:\n","\n","- _Dropping a column_\n","- _Changing an existing column's data type (in place)_\n","- _Renaming column names that differ only by case (e.g. “Foo” and “foo”)_\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0613cdd3-a0c0-48b2-b5de-b23b93af8938"},{"cell_type":"code","source":["new_device.write.format(\"delta\") \\\n","                .mode(\"append\") \\\n","                .option(\"mergeSchema\", True) \\\n","                .saveAsTable(\"demo.device\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"editable":false},"id":"b369c22b-15ec-4453-be13-b9f6641efb07"},{"cell_type":"markdown","source":["Showing the schema evolution"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"febf0301-8cef-4aaf-ac13-18e4c0c9b62c"},{"cell_type":"code","source":["%%sql \n","\n","DESCRIBE TABLE demo.device"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false,"run_control":{"frozen":false},"editable":false},"id":"7d00a941-6646-488f-b167-6083f6c9f825"},{"cell_type":"markdown","source":["## Enable autoMerge\n","\n","Setting **_mergeSchema_** to true every time you'd like to write with a mismatched schema can be tedious. Let's look at how to enable schema evolution by default.\n","\n","You can also set a spark property that will enable **autoMerge** by default. Once this property is set, you don't need to manually set **_mergeSchema_** to true when writing data with a different schema to a delta table!\n","\n","Use **spark.databricks.delta.schema.autoMerge** equal to **true** to enable it and become default setting  spark configuration will\n","\n","\n","> **Warning**\n","> Use with caution, as schema enforcement **_will no longer warn you about unintended schema mismatches_**.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"60bff354-7e67-4c77-b654-1acc284ed7a7"},{"cell_type":"code","source":["spark.conf.get(\"spark.databricks.delta.schema.autoMerge.enabled\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"editable":true},"id":"f7492ca7-6ad9-4b93-97ac-cdfb17dc5aa9"},{"cell_type":"markdown","source":["> You can enable schema evolution by default by setting **autoMerge** to **true**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"593da1bb-b16d-44f1-8034-871fd0a39afb"},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"editable":false},"id":"6612f225-0cea-49d8-9bf9-bf893b2684d5"},{"cell_type":"markdown","source":["Let's create a dataframe with an entirely different schema from the existing Delta table and see what happens when it's appended."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d9f6b5b2-8b2b-4cd1-862d-125e8063e63d"},{"cell_type":"code","source":["deviceSchema = StructType([StructField(\"action\", StringType(), False),\n","  StructField(\"status\", StringType(), False) # new column\n","  ])\n","\n","data = [\n","        ('Done', \"Good\")\n","    ]  \n","\n","new_device = spark.createDataFrame(data=data,schema=deviceSchema)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"editable":false},"id":"86243db2-568b-49b4-9163-c9fe80ae8605"},{"cell_type":"markdown","source":["Let's append a single column dataframe to the delta table to illustrate"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ca0b56d7-2cd8-4f2c-88de-4e759b67b778"},{"cell_type":"code","source":["## .option(\"mergeSchema\", True) is not needed anymore\n","\n","new_device.write.format(\"delta\") \\\n","                .mode(\"append\") \\\n","                .saveAsTable(\"demo.device\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"editable":false},"id":"fe7ab47a-2cff-421c-8a68-dfba1ad948ea"},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"false\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"1563ad52-37ff-47e2-a053-ec15dc23febb"},{"cell_type":"markdown","source":["## Why is schema evolution so import?\n","\n","Schema evolution can be used anytime you intend  to change the schema of your tables (as opposed to where you accidentally added columns to your dataframe that shouldn't be there). It's the easiest way to migrate your schema because it automatically adds the correct column names and data types, without having to declare them explicitly."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5785bdf6-5031-4fcb-8076-30d4aea9e2d7"},{"cell_type":"code","source":["%%sql \n","\n","DESCRIBE TABLE demo.device"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false,"run_control":{"frozen":false},"editable":false},"id":"f573fe52-a662-4f99-ade4-8bed3c1ea562"},{"cell_type":"code","source":["%%sql \n","\n","SELECT * FROM demo.device"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false,"run_control":{"frozen":false},"editable":false},"id":"026566ed-6a93-4617-acbc-508a63ec363d"},{"cell_type":"markdown","source":["## Explicitly update schema \n","\n","Change column type or name"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e468a905-0258-4fe1-a6cc-08d28204b33f"},{"cell_type":"code","source":["df = spark.read.table(\"demo.device\").withColumn(\"device_id\", col(\"device_id\").cast(\"string\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"editable":false},"id":"903f3261-fcc0-4b9e-b980-addd1fb01147"},{"cell_type":"code","source":["df.printSchema()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"34010cd7-ed7c-4ba6-9b7f-959f8020d6b3"},{"cell_type":"code","source":["df.write.format(\"delta\") \\\n","                .mode(\"overwrite\") \\\n","                .option(\"overwriteschema\", True) \\\n","                .saveAsTable(\"demo.device\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"editable":true},"id":"7c38cad5-64f7-4a27-837e-c399cb0423ba"},{"cell_type":"code","source":["%%sql \n","\n","DESCRIBE TABLE demo.device"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"},"collapsed":false,"run_control":{"frozen":false},"editable":false},"id":"ccfe0d83-05ba-47fc-b617-d96f5a95a021"},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS demo.device\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"editable":true,"run_control":{"frozen":false}},"id":"470d2e26-3c1b-491e-9fa1-2a6aff9985ed"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"notebook_environment":{},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}